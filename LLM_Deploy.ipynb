{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama and pyngrok\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install pyngrok flask requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quoloUqUZIRu",
        "outputId": "e221f5a8-31a5-4446-b2dd-7abf4ecde734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import time, os\n",
        "\n",
        "# ðŸ”§ USER SETTINGS\n",
        "NGROK_AUTHTOKEN = \"2ygjBSFtr1u6JbaEGjN6rK0vxie_3VzUt6MbZ55URyvNwm6nS\"   # @param {type:\"string\"}\n",
        "NGROK_DOMAIN    = \"hugely-chief-dingo.ngrok-free.app\"   # @param {type:\"string\"}\n",
        "API_KEY         = \"sec\"  # @param {type:\"string\"}\n",
        "MODEL           = \"qwen2.5:7b-instruct\"\n",
        "\n",
        "if not NGROK_AUTHTOKEN:\n",
        "    raise ValueError(\"Enter your ngrok authtoken!\")\n",
        "if not NGROK_DOMAIN:\n",
        "    raise ValueError(\"Enter your reserved NGROK_DOMAIN!\")\n",
        "\n",
        "# Configure ngrok region\n",
        "conf.get_default().region = \"eu\"\n",
        "\n",
        "# Login to ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# Kill old processes\n",
        "!pkill -f ollama || echo \"ok\"\n",
        "!fuser -k 8000/tcp || echo \"ok\"\n",
        "\n",
        "# Start Ollama daemon\n",
        "get_ipython().system_raw(\"OLLAMA_HOST=0.0.0.0 ollama serve &\")\n",
        "\n",
        "print(\"ðŸ§  Warming Ollama...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Pull model\n",
        "!ollama pull $MODEL\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyDWS1iRZJ-E",
        "outputId": "dbabfb19-6b2b-461f-919e-ed8120f21791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "8000/tcp:             2382\n",
            "ðŸ§  Warming Ollama...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile api.py\n",
        "from flask import Flask, request, jsonify\n",
        "import requests as http\n",
        "\n",
        "API_KEY = \"sec\"\n",
        "MODEL   = \"qwen2.5:7b-instruct\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# ----------------------------------------\n",
        "# ðŸ” 1) MODELS LIST ENDPOINT (OpenAI style)\n",
        "# ----------------------------------------\n",
        "@app.route(\"/v1/models\", methods=[\"GET\"])\n",
        "def list_models():\n",
        "    try:\n",
        "        r = http.get(\"http://localhost:11434/api/tags\")\n",
        "        if r.status_code != 200:\n",
        "            return jsonify({\n",
        "                \"object\": \"list\",\n",
        "                \"data\": [],\n",
        "                \"error\": \"could not load models from ollama\"\n",
        "            }), 500\n",
        "\n",
        "        ollama_models = r.json().get(\"models\", [])\n",
        "\n",
        "        models_out = [\n",
        "            {\n",
        "                \"id\": m[\"name\"],\n",
        "                \"object\": \"model\",\n",
        "                \"owned_by\": \"ollama\",\n",
        "                \"permission\": []\n",
        "            }\n",
        "            for m in ollama_models\n",
        "        ]\n",
        "\n",
        "        return jsonify({\n",
        "            \"object\": \"list\",\n",
        "            \"data\": models_out\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# ðŸ’¬ 2) CHAT COMPLETIONS (OpenAI-compatible wrapper)\n",
        "# -----------------------------------------------------\n",
        "@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n",
        "def completions():\n",
        "    # --- API key validation ---\n",
        "    auth = request.headers.get(\"Authorization\", \"\").replace(\"Bearer \", \"\")\n",
        "    if auth != API_KEY:\n",
        "        return jsonify({\"error\": \"unauthorized\"}), 401\n",
        "\n",
        "    data = request.json\n",
        "    user_msg = data[\"messages\"][0][\"content\"]\n",
        "\n",
        "    # --- Send request to Ollama (non-streaming) ---\n",
        "    r = http.post(\n",
        "        \"http://localhost:11434/api/chat\",\n",
        "        json={\n",
        "            \"model\": MODEL,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": user_msg}],\n",
        "            \"stream\": False\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if r.status_code != 200:\n",
        "        return jsonify({\n",
        "            \"error\": \"ollama_error\",\n",
        "            \"details\": r.text\n",
        "        }), 500\n",
        "\n",
        "    reply = r.json()[\"message\"][\"content\"]\n",
        "\n",
        "    # --- Return OpenAI-style structure ---\n",
        "    return jsonify({\n",
        "        \"id\": \"chatcmpl-ollama\",\n",
        "        \"object\": \"chat.completion\",\n",
        "        \"model\": MODEL,\n",
        "        \"choices\": [\n",
        "            {\n",
        "                \"index\": 0,\n",
        "                \"message\": {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": reply\n",
        "                },\n",
        "                \"finish_reason\": \"stop\"\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# ðŸš€ 3) API Runner\n",
        "# -----------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸš€ API running at http://0.0.0.0:8000\")\n",
        "    app.run(host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bag1CuYaZMCM",
        "outputId": "b94f57cf-6c63-49ab-ef8b-2abeeaf2b73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill old server\n",
        "!pkill -f api.py || echo \"ok\"\n",
        "!fuser -k 8000/tcp || echo \"ok\"\n",
        "\n",
        "# Start Flask server permanently\n",
        "!nohup python3 api.py >/dev/null 2>&1 &\n",
        "\n",
        "# Wait for startup\n",
        "import time; time.sleep(3)\n",
        "\n",
        "# Verify port\n",
        "!lsof -i:8000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMJWBGzkZOLD",
        "outputId": "a709900e-6b01-496f-b899-643a620d78a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "ok\n",
            "COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 4421 root    3u  IPv4 129447      0t0  TCP *:8000 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(\n",
        "    addr=8000,\n",
        "    proto=\"http\",\n",
        "    hostname=NGROK_DOMAIN\n",
        ")\n",
        "\n",
        "print(\"=======================================\")\n",
        "print(\"ðŸš€ API is LIVE at:\", public_url)\n",
        "print(\"ðŸ”‘ API KEY:\", API_KEY)\n",
        "print(\"=======================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNminLGEZQFs",
        "outputId": "629dbf80-43ce-40c0-96ac-40ba3b0865b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================\n",
            "ðŸš€ API is LIVE at: NgrokTunnel: \"https://hugely-chief-dingo.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "ðŸ”‘ API KEY: sec\n",
            "=======================================\n"
          ]
        }
      ]
    }
  ]
}